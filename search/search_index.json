{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Full Stack Deep Learning Our mission is to help you go from a promising ML experiment to a shipped product, with real-world impact. Current Course We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course, with all lectures and labs available for free. \ud83d\ude80Spring 2021 Online Course\ud83d\ude80 About this course There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale Who is this for The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review. Instructors Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp Course Offerings \ud83d\ude80Spring 2021\ud83d\ude80 Online Course Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Home"},{"location":"#current-course","text":"We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course, with all lectures and labs available for free. \ud83d\ude80Spring 2021 Online Course\ud83d\ude80","title":"Current Course"},{"location":"#about-this-course","text":"There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale","title":"About this course"},{"location":"#who-is-this-for","text":"The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review.","title":"Who is this for"},{"location":"#instructors","text":"Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp","title":"Instructors"},{"location":"#course-offerings","text":"\ud83d\ude80Spring 2021\ud83d\ude80 Online Course Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Course Offerings"},{"location":"spring2021/","text":"Full Stack Deep Learning - Spring 2021 We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details . Week 1: Fundamentals The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro Reading: How the backpropagation algorithm works Week 2: CNNs The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs Reading: A brief introduction to Neural Style Transfer Improving the way neural networks learn Week 3: RNNs The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs Lab 3: RNNs Reading: The Unreasonable Effectiveness of Recurrent Neural Networks Attention Craving RNNS: Building Up To Transformer Networks Week 4: Transformers The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers Reading: Transformers from Scratch Week 5: ML Projects The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects \u261d\ufe0fWith detailed notes! Reading: Rules of Machine Learning Those in the syncronous online course will have their first weekly assignment: Assignment 1 , available on Gradescope. Week 6: Infra & Tooling The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. Lecture 6: Infrastructure & Tooling Reading: Machine Learning: The High-Interest Credit Card of Technical Debt Those in the syncronous online course will have to work on Assignment 2 . Week 7: Troubleshooting The week of March 14, we talk about how to best troubleshoot training. In lab, we learn to manage experiments. Lecture 7: Troubleshooting DNNs Lab 5: Experiment Management Reading: Why is machine learning hard? Those in the syncronous online course will have to work on Assignment 3 . Week 8: Data The week of March 21, we talk about Data Management, and label some data in lab. Lecture 8: Data Management Lab 6: Data Management Reading: Emerging architectures for modern data infrastructure Those in the syncronous online course will have to work on Assignment 4 . Week 9: Ethics The week of March 28, we discuss ethical considerations. In lab, we move from lines to paragraphs. Lecture 9: AI Ethics Lab 7: Line Detection or Paragraph Recognition Those in the synchronous online course will have to submit their project proposals . Week 10: Testing The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability Lab 8: Testing & CI Those in the synchronous online course will work on their projects. Week 11: Deployment The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring Lab 9: Web Deployment Those in the synchronous online course will work on their projects. Week 12: Research The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions Lab 10: Monitoring Those in the synchronous online course will work on their projects. Week 13: Teams The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups. Week 14: Projects The week of May 3, we watch the best course project videos together, and give out awards. There are rumors of a fun conference in the air, too... Other Resources Fast.ai is a great free two-course sequence aimed at first getting hackers to train state-of-the-art models as quickly as possible, and only afterward delving into how things work under the hood. Highly recommended for anyone. Dive Into Deep Learning is a great free textbook with Jupyter notebooks for every part of deep learning. NYU\u2019s Deep Learning course has excellent PyTorch breakdowns of everything important going on in deep learning. Stanford\u2019s ML Systems Design course has lectures that parallel those in this course. The Batch by Andrew Ng is a great weekly update on progress in the deep learning world. /r/MachineLearning/ is the best community for staying up to date with the latest developments.","title":"Spring 2021 Schedule"},{"location":"spring2021/#full-stack-deep-learning-spring-2021","text":"We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details .","title":"Full Stack Deep Learning - Spring 2021"},{"location":"spring2021/#week-1-fundamentals","text":"The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro Reading: How the backpropagation algorithm works","title":"Week 1: Fundamentals"},{"location":"spring2021/#week-2-cnns","text":"The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs Reading: A brief introduction to Neural Style Transfer Improving the way neural networks learn","title":"Week 2: CNNs"},{"location":"spring2021/#week-3-rnns","text":"The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs Lab 3: RNNs Reading: The Unreasonable Effectiveness of Recurrent Neural Networks Attention Craving RNNS: Building Up To Transformer Networks","title":"Week 3: RNNs"},{"location":"spring2021/#week-4-transformers","text":"The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers Reading: Transformers from Scratch","title":"Week 4: Transformers"},{"location":"spring2021/#week-5-ml-projects","text":"The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects \u261d\ufe0fWith detailed notes! Reading: Rules of Machine Learning Those in the syncronous online course will have their first weekly assignment: Assignment 1 , available on Gradescope.","title":"Week 5: ML Projects"},{"location":"spring2021/#week-6-infra-tooling","text":"The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. Lecture 6: Infrastructure & Tooling Reading: Machine Learning: The High-Interest Credit Card of Technical Debt Those in the syncronous online course will have to work on Assignment 2 .","title":"Week 6: Infra &amp; Tooling"},{"location":"spring2021/#week-7-troubleshooting","text":"The week of March 14, we talk about how to best troubleshoot training. In lab, we learn to manage experiments. Lecture 7: Troubleshooting DNNs Lab 5: Experiment Management Reading: Why is machine learning hard? Those in the syncronous online course will have to work on Assignment 3 .","title":"Week 7: Troubleshooting"},{"location":"spring2021/#week-8-data","text":"The week of March 21, we talk about Data Management, and label some data in lab. Lecture 8: Data Management Lab 6: Data Management Reading: Emerging architectures for modern data infrastructure Those in the syncronous online course will have to work on Assignment 4 .","title":"Week 8: Data"},{"location":"spring2021/#week-9-ethics","text":"The week of March 28, we discuss ethical considerations. In lab, we move from lines to paragraphs. Lecture 9: AI Ethics Lab 7: Line Detection or Paragraph Recognition Those in the synchronous online course will have to submit their project proposals .","title":"Week 9: Ethics"},{"location":"spring2021/#week-10-testing","text":"The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability Lab 8: Testing & CI Those in the synchronous online course will work on their projects.","title":"Week 10: Testing"},{"location":"spring2021/#week-11-deployment","text":"The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring Lab 9: Web Deployment Those in the synchronous online course will work on their projects.","title":"Week 11: Deployment"},{"location":"spring2021/#week-12-research","text":"The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions Lab 10: Monitoring Those in the synchronous online course will work on their projects.","title":"Week 12: Research"},{"location":"spring2021/#week-13-teams","text":"The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups.","title":"Week 13: Teams"},{"location":"spring2021/#week-14-projects","text":"The week of May 3, we watch the best course project videos together, and give out awards. There are rumors of a fun conference in the air, too...","title":"Week 14: Projects"},{"location":"spring2021/#other-resources","text":"Fast.ai is a great free two-course sequence aimed at first getting hackers to train state-of-the-art models as quickly as possible, and only afterward delving into how things work under the hood. Highly recommended for anyone. Dive Into Deep Learning is a great free textbook with Jupyter notebooks for every part of deep learning. NYU\u2019s Deep Learning course has excellent PyTorch breakdowns of everything important going on in deep learning. Stanford\u2019s ML Systems Design course has lectures that parallel those in this course. The Batch by Andrew Ng is a great weekly update on progress in the deep learning world. /r/MachineLearning/ is the best community for staying up to date with the latest developments.","title":"Other Resources"},{"location":"spring2021/lab-1/","text":"Lab 1: Setup and Introduction Video In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST Slides PDF Download Follow Along GitHub Readme","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#lab-1-setup-and-introduction","text":"","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#video","text":"In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST","title":"Video"},{"location":"spring2021/lab-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lab-1/#follow-along","text":"GitHub Readme","title":"Follow Along"},{"location":"spring2021/lab-2/","text":"Lab 2: CNNs and Synthetic Data Video No slides. Follow Along GitHub Readme In this lab, you train a single-line ConvNet predictor on the EMNIST dataset and then synthetically generate your own data. 00:00 - Introduction 05:23 - Look at the EMNIST dataset 09:52 - Train a base ConvNet model 12:43 - Examine the ConvNet code 17:33 - Lab 2 homework 19:35 - Make a synthetic dataset of EMNIST lines","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#lab-2-cnns-and-synthetic-data","text":"","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#video","text":"No slides.","title":"Video"},{"location":"spring2021/lab-2/#follow-along","text":"GitHub Readme In this lab, you train a single-line ConvNet predictor on the EMNIST dataset and then synthetically generate your own data. 00:00 - Introduction 05:23 - Look at the EMNIST dataset 09:52 - Train a base ConvNet model 12:43 - Examine the ConvNet code 17:33 - Lab 2 homework 19:35 - Make a synthetic dataset of EMNIST lines","title":"Follow Along"},{"location":"spring2021/lab-3/","text":"Lab 3: RNNs Video No slides Follow along Readme Notes 00:00 - Introduction. 01:59 - Introduce LineCNNSimple, a model that can read multiple characters in an image. 15:52 - Make this model more efficient with LineCNN, which uses a fully convolutional network. 18:18 - Upgrade the model further into LitModelCTC, which uses a CTC (Connectionist Temporal Classification) loss. 23:29 - Finalize your model, LineCNNLSTM, by adding an LSTM layer on top of CNN. 27:34 - Lab 3 homework.","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#lab-3-rnns","text":"","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-3/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-3/#notes","text":"00:00 - Introduction. 01:59 - Introduce LineCNNSimple, a model that can read multiple characters in an image. 15:52 - Make this model more efficient with LineCNN, which uses a fully convolutional network. 18:18 - Upgrade the model further into LitModelCTC, which uses a CTC (Connectionist Temporal Classification) loss. 23:29 - Finalize your model, LineCNNLSTM, by adding an LSTM layer on top of CNN. 27:34 - Lab 3 homework.","title":"Notes"},{"location":"spring2021/lab-4/","text":"Lab 4: Transformers Video No slides Follow along Readme Notes In this lab, you use the LineCNN + LSTM model with CTC loss from lab 3 as an \"encoder\" of the image, and then send it through Transformer decoder layers. 00:00 - Introduction 01:43 - LineCNNTransformer class 04:50 - TransformerLitModel 06:51 - Code to make predictions 08:50 - Training guidelines","title":"Lab 4: Transformers"},{"location":"spring2021/lab-4/#lab-4-transformers","text":"","title":"Lab 4: Transformers"},{"location":"spring2021/lab-4/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-4/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-4/#notes","text":"In this lab, you use the LineCNN + LSTM model with CTC loss from lab 3 as an \"encoder\" of the image, and then send it through Transformer decoder layers. 00:00 - Introduction 01:43 - LineCNNTransformer class 04:50 - TransformerLitModel 06:51 - Code to make predictions 08:50 - Training guidelines","title":"Notes"},{"location":"spring2021/lab-5/","text":"Lab 5: Experiment Management Video No slides Follow along Readme Notes","title":"Lab 5: Experiment Management"},{"location":"spring2021/lab-5/#lab-5-experiment-management","text":"","title":"Lab 5: Experiment Management"},{"location":"spring2021/lab-5/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-5/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-5/#notes","text":"","title":"Notes"},{"location":"spring2021/lecture-1/","text":"Lecture 1: DL Fundamentals Video Slides PDF Download Notes In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#lecture-1-dl-fundamentals","text":"","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#video","text":"","title":"Video"},{"location":"spring2021/lecture-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-1/#notes","text":"In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Notes"},{"location":"spring2021/lecture-2a/","text":"Lecture 2A: CNNs Video Slides PDF Download Notes In this video, we first review convolution operation, the most basic property of Convolutional Neural Networks. Then, we look at other important operations for ConvNets. Finally, we transition to looking at a classic ConvNet architecture called LeNet. 00:00 - Introduction 01:08 - Convolutional Filters 07:10 - Filter Stacks and ConvNets 11:25 - Strides and Padding 14:35 - Filter Math 21:44 - Convolution Implementation Notes 24:04 - Increasing the Receptive Field with Dilated Convolutions 27:30 - Decreasing the Tensor Size with Pooling and 1x1-Convolutions 30:54 - LeNet Architecture","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#lecture-2a-cnns","text":"","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2a/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2a/#notes","text":"In this video, we first review convolution operation, the most basic property of Convolutional Neural Networks. Then, we look at other important operations for ConvNets. Finally, we transition to looking at a classic ConvNet architecture called LeNet. 00:00 - Introduction 01:08 - Convolutional Filters 07:10 - Filter Stacks and ConvNets 11:25 - Strides and Padding 14:35 - Filter Math 21:44 - Convolution Implementation Notes 24:04 - Increasing the Receptive Field with Dilated Convolutions 27:30 - Decreasing the Tensor Size with Pooling and 1x1-Convolutions 30:54 - LeNet Architecture","title":"Notes"},{"location":"spring2021/lecture-2b/","text":"Lecture 2B: Computer Vision Video Slides PDF Download Notes In this video, we will review notable applications of deep learning in computer vision. First, we will tour some ConvNet architectures. Then, we will talk about localization, detection, and segmentation problems. We will conclude with more advanced methods. Learn more at this website: https://paperswithcode.com/area/computer-vision 00:00 - Introduction 02:51 - AlexNet 05:09 - ZFNet 06:54 - VGGNet 09:06 - GoogLeNet 11:57 - ResNet 15:15 - SqueezeNet 17:05 - Architecture Comparisons 20:00 - Localization, Detection, and Segmentation Tasks 24:00 - Overfeat, YOLO, and SSD Methods 28:01 - Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net) 34:33 - Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation) 37:00 - Adversarial Attacks 40:56 - Style Transfer","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#lecture-2b-computer-vision","text":"","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2b/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2b/#notes","text":"In this video, we will review notable applications of deep learning in computer vision. First, we will tour some ConvNet architectures. Then, we will talk about localization, detection, and segmentation problems. We will conclude with more advanced methods. Learn more at this website: https://paperswithcode.com/area/computer-vision 00:00 - Introduction 02:51 - AlexNet 05:09 - ZFNet 06:54 - VGGNet 09:06 - GoogLeNet 11:57 - ResNet 15:15 - SqueezeNet 17:05 - Architecture Comparisons 20:00 - Localization, Detection, and Segmentation Tasks 24:00 - Overfeat, YOLO, and SSD Methods 28:01 - Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net) 34:33 - Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation) 37:00 - Adversarial Attacks 40:56 - Style Transfer","title":"Notes"},{"location":"spring2021/lecture-3/","text":"Lecture 3: RNNs Video Slides PDF Download Notes 00:00 - Introduction 01:34 - Sequence Problems 06:28 - Review of RNNs 22:00 - Vanishing Gradient Issue 27:52 - LSTMs and Its Variants 34:10 - Bidirectionality and Attention from Google's Neural Machine Translation 46:38 - CTC Loss 52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures 54:55 - WaveNet","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#lecture-3-rnns","text":"","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#video","text":"","title":"Video"},{"location":"spring2021/lecture-3/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-3/#notes","text":"00:00 - Introduction 01:34 - Sequence Problems 06:28 - Review of RNNs 22:00 - Vanishing Gradient Issue 27:52 - LSTMs and Its Variants 34:10 - Bidirectionality and Attention from Google's Neural Machine Translation 46:38 - CTC Loss 52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures 54:55 - WaveNet","title":"Notes"},{"location":"spring2021/lecture-4/","text":"Lecture 4: Transformers Video Slides PDF Download Notes In this video, you will learn about the origin of transfer learning in computer vision, its application in NLP in the form of embedding, NLP's ImageNet moment, and the Transformers model families. 00:00 - Introduction 00:42 - Transfer Learning in Computer Vision 04:00 - Embeddings and Language Models 10:09 - NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE 16:49 - Rise of Transformers 18:20 - Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization 27:33 - Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc. 36:20 - GPT3 Demos 42:53 - Future Directions","title":"Lecture 4: Transformers"},{"location":"spring2021/lecture-4/#lecture-4-transformers","text":"","title":"Lecture 4: Transformers"},{"location":"spring2021/lecture-4/#video","text":"","title":"Video"},{"location":"spring2021/lecture-4/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-4/#notes","text":"In this video, you will learn about the origin of transfer learning in computer vision, its application in NLP in the form of embedding, NLP's ImageNet moment, and the Transformers model families. 00:00 - Introduction 00:42 - Transfer Learning in Computer Vision 04:00 - Embeddings and Language Models 10:09 - NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE 16:49 - Rise of Transformers 18:20 - Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization 27:33 - Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc. 36:20 - GPT3 Demos 42:53 - Future Directions","title":"Notes"},{"location":"spring2021/lecture-5/","text":"Lecture 5: ML Projects Learn how to set up Machine Learning projects like a pro. This includes an understanding of the ML lifecycle, an acute mind of the feasibility and impact, an awareness of the project archetypes, and an obsession with metrics and baselines. Video Slides PDF Download Detailed Notes By James Le and Vishnu Rachakonda 1 - Why Do ML Projects Fail? Based on a report from TechRepublic a few years back, despite increased interest in adopting machine learning (ML) in the enterprise, 85% of machine learning projects ultimately fail to deliver on their intended promises to business. Failure can happen for many reasons; however, a few glaring dangers will cause any AI project to crash and burn. ML is still very much a research endeavor. Therefore it is very challenging to aim for a 100% success rate. Many ML projects are technically infeasible or poorly scoped. Many ML projects never leap production, thus getting stuck at the prototype phase. Many ML projects have unclear success criteria because of a lack of understanding of the value proposition. Many ML projects are poorly managed because of a lack of interest from leadership. 2 - Lifecycle It\u2019s essential to understand what constitutes all of the activities in a machine learning project. Typically speaking, there are four major phases: Planning and Project Setup : At this phase, we want to decide the problem to work on, determine the requirements and goals, figure out how to allocate resources properly, consider the ethical implications, etc. Data Collection and Labeling : At this phase, we want to collect training data and potentially annotate them with ground truth, depending on the specific sources where they come from. We may find that it\u2019s too hard to get the data, or it might be easier to label for a different task. If that\u2019s the case, go back to phase 1. Model Training and Model Debugging : At this phase, we want to implement baseline models quickly, find and reproduce state-of-the-art methods for the problem domain, debug our implementation, and improve the model performance for specific tasks. We may realize that we need to collect more data or that labeling is unreliable (thus, go back to phase 2). Or we may recognize that the task is too challenging and there is a tradeoff between project requirements (thus, go back to phase 1). Model Deploying and Model Testing : At this phase, we want to pilot the model in a constrained environment (i.e., in the lab), write tests to prevent regressions, and roll the model into production. We may see that the model doesn\u2019t work well in the lab, so we want to keep improving the model\u2019s accuracy (thus, go back to phase 3). Or we may want to fix the mismatch between training data and production data by collecting more data and mining hard cases (thus go back to phase 2). Or we may find out that the metric picked doesn\u2019t actually drive downstream user behavior, and/or performance in the real world isn\u2019t great. In such situations, we want to revisit the projects\u2019 metrics and requirements (thus, go back to phase 1). Besides the per-project activities mentioned above, there are two other things that any ML team will need to solve across any projects they get involved with: (1) building the team and hiring people; and (2) setting up infrastructure and tooling to build ML systems repeatedly and at scale. Additionally, it might be useful to understand state-of-the-art results in your application domain so that you know what\u2019s possible and what to try next. 3 - Prioritizing Projects To prioritize projects to work on, you want to find high-impact problems and assess the potential costs associated with them. The picture below shows a general framework that encourages us to target projects with high impact and high feasibility. High Impact There are no silver bullets to find high-impact ML problems to work on, but here are a few useful mental models: Where can you take advantage of cheap prediction? Where is there friction in your product? Where can you automate complicated manual processes? What are other people doing? Cheap Prediction In the book \u201c Prediction Machines ,\u201d the authors (Ajay Agrawal, Joshua Gans, and Avi Goldfarb) come up with an excellent mental model on the economics of Artificial Intelligence: As AI reduces the cost of prediction and prediction is central for decision making, cheap predictions would be universal for problems across business domains . Therefore, you should look for projects where cheap predictions will have a huge business impact. Product Needs Another lens is to think about what your product needs. In the article \u201c Three Principles for Designing ML-Powered Products ,\u201d the Spotify Design team emphasizes the importance of building ML from a product perspective and looking for parts of the product experience with high friction . Automating those parts is exactly where there is a lot of impact for ML to make your business better. ML Strength In his popular blog post \u201c Software 2.0 ,\u201d Andrej Karpathy contrasts software 1.0 (which are traditional programs with explicit instructions) and software 2.0 (where humans specify goals, while the algorithm searches for a program that works). Software 2.0 programmers work with datasets, which get compiled via optimization\u200a\u2014\u200awhich works better, more general, and less computationally expensive. Therefore, you should look for complicated rule-based software where we can learn the rules instead of programming them. Inspiration From Others Instead of reinventing the wheel, you can look at what other companies are doing. In particular, check out papers from large frontier organizations (Google, Facebook, Nvidia, Netflix, etc.) and blog posts from top earlier-stage companies (Uber, Lyft, Spotify, Stripe, etc.). Here is a list of excellent ML use cases to check out (credit to Chip Huyen\u2019s ML Systems Design Lecture 2 Note ): Human-Centric Machine Learning Infrastructure at Netflix (Ville Tuulos, InfoQ 2019) 2020 state of enterprise machine learning (Algorithmia, 2020) Using Machine Learning to Predict Value of Homes On Airbnb (Robert Chang, Airbnb Engineering & Data Science, 2017) Using Machine Learning to Improve Streaming Quality at Netflix (Chaitanya Ekanadham, Netflix Technology Blog, 2018) 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com (Bernardi et al., KDD, 2019) How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach (Gabriel Aldamiz, HackerNoon, 2018) Machine Learning-Powered Search Ranking of Airbnb Experiences (Mihajlo Grbovic, Airbnb Engineering & Data Science, 2019) From shallow to deep learning in fraud (Hao Yi Ong, Lyft Engineering, 2018) Space, Time and Groceries (Jeremy Stanley, Tech at Instacart, 2017) Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning (Brad Neuberg, Dropbox Engineering, 2017) Scaling Machine Learning at Uber with Michelangelo (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019) Spotify\u2019s Discover Weekly: How machine learning finds your new music (Sophia Ciocca, 2017) High Feasibility The three primary cost drivers of ML projects in order of importance are data availability, accuracy requirement, and problem difficulty. Data Availability Here are the questions you need to ask concerning the data availability: How hard is it to acquire data? How expensive is data labeling? How much data will be needed? How stable is the data? What are the data security requirements? Accuracy Requirement Here are the questions you need to ask concerning the accuracy requirement: How costly are wrong predictions? How frequently does the system need to be right to be useful? What are the ethical implications? It is worth noting that ML project costs tend to scale super-linearly in the accuracy requirement. The fundamental reason is that you typically need a lot more data and more high-quality labels to achieve high accuracy numbers. Problem Difficulty Here are the questions you need to ask concerning the problem difficulty: Is the problem well-defined? Is there good published work on similar problems? What are the computing requirements? Can a human do it? So what\u2019s still hard in machine learning? As a caveat, it\u2019s historically very challenging to predict what types of problems will be difficult for ML to solve in the future. But generally speaking, both unsupervised learning and reinforcement learning are still hard, even though they show promise in limited domains where tons of data and compute are available. Zooming into supervised learning , here are three types of hard problems: Output is complex: These are problems where the output is high-dimensional or ambiguous. Examples include 3D reconstruction, video prediction, dialog systems, open-ended recommendation systems, etc. Reliability is required: These are problems where high precision and robustness are required. Examples include systems that can fail safely in out-of-distribution scenarios, is robust to adversarial attacks, or needs to tackle highly precise tasks. Generalization is required: These are problems with out-of-distribution data or in the domains of reasoning, planning, and causality. Examples include any systems for self-driving vehicles or any systems that deal with small data. Finally, this is a nice checklist for you to run an ML feasibility assessment: Are you sure that you need ML at all? Put in the work upfront to define success criteria with all of the stakeholders. Consider the ethics of using ML. Do a literature review. Try to build a labeled benchmark dataset rapidly. Build a minimal viable product with manual rules Are you \u201creally sure\u201d that you need ML at all? 4 - Archetypes So far, we\u2019ve talked about the lifecycle and the impact of all machine learning projects. Ultimately, we generally want these projects, or applications of machine learning, to be useful for products. As we consider how ML can be applied in products, it\u2019s helpful to note that there are common machine learning product archetypes or recurrent patterns through which machine learning is applied to products. You can think of these as \u201cmental models\u201d you can use to assess your project and easily prioritize the needed resources. There are three common archetypes in machine learning projects: Software 2.0 , Human-in-the-loop , and autonomous systems . They are shown in the table below, along with common examples and questions. We\u2019ll dive deeper into each. Archetype Examples Questions Software 2.0 - Improve code completion in IDE - Build customized recommendation system - Build a better video game AI - Do your models truly improve performance? - Does performance improvement generate business value? - Do performance improvements lead to a data flywheel? Human-in-the-loop - Turn sketches into slides - Email auto-completion - Help radiologists do job faster - How good does the system need to be to be useful? - How can you collect enough data to make it good? Autonomous Systems - Full self-driving - Automated customer support - Automated website design - What is an acceptable failure rate for the system? - How can you guarantee that it won\u2019t exceed the failure rate? - How inexpensively can you label data from the system? Software 2.0 Software 2.0, which we previously alluded to from the Karpathy article , is defined as \u201c augmenting existing rules-based or deterministic software with machine learning, a probabilistic approach .\u201d Examples of this are taking a code completer in an IDE and improving the experience for the user by adding an ML component. Rather than suggesting a command based solely on the leading characters the programmer has written, you might add a model that suggests commands based on previous commands the programmer has written. As you build a software 2.0 project, strongly consider the concept of the data flywheel . For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It\u2019s a classic virtuous cycle and truly the gold standard for ML projects. In embarking on creating a data flywheel, critically consider where the model could fail in relation to your product. For example, do more users lead to collecting more data that is useful for improving your model? An actual system needs to be set up to capture this data and ensure that it's meaningful for the ML lifecycle. Furthermore, consider whether more data will lead to a better model (your job as an ML practitioner) or whether a better model and better predictions will actually lead to making the product better. Ideally, you should have a quantitative assessment of what makes your product \u201cbetter\u201d and map model improvement to it. Human-in-the-Loop (HIL) HIL systems are defined as machine learning systems where the output of your model will be reviewed by a human before being executed in the real world . For example, consider translating sketches into slides. An ML algorithm can take a sketch\u2019s input and suggest to a user a particular slide design. Every output of the ML model is considered and executed upon by a human, who ultimately has to decide on the slide\u2019s design. Autonomous Systems Autonomous systems are defined as machine learning systems where the system itself makes decisions or engages in outputs that are almost never reviewed by a human . Canonically, consider the self-driving car! Feasibility Let\u2019s discuss how the product archetypes relate back to project priority. In terms of feasibility and impact, the two axes on which we consider priority, software 2.0 tends to have high feasibility but potentially low impact. The existing system is often being optimized rather than wholly replaced. However, this status with respect to priority is not static by any means. Building a data flywheel into your software 2.0 project can improve your product\u2019s impact by improving the model\u2019s performance on the task and future ones. In the case of human-in-the-loop systems, their feasibility and impact sit squarely in between autonomous systems and software 2.0. HIL systems, in particular, can benefit disproportionately in their feasibility and impact from effective product design, which naturally takes into account how humans interact with technology and can mitigate risks for machine learning model behavior. Consider how the Facebook photo tagging algorithm is implemented. Rather than tagging the user itself, the algorithm frequently asks the user to tag themselves. This effective product design allows the model to perform more effectively in the user\u2019s eye and reduces the impact of false classifications. Grammarly similarly solicits user input as part of its product design through offering explanations. Finally, recommender systems also implement this idea. In general, good product design can smooth the rough edges of ML (check out the concept of designing collaborative AI ). There are industry-leading resources that can help you merge product design and ML. Apple\u2019s ML product design guidelines suggest three key questions to anyone seeking to put ML into a product: What role does ML play in your product? How can you learn from your users? How should your app handle mistakes? Associated with each question is a set of design paradigms that help address the answers to each question. There are similarly great resources from Microsoft and Spotify . Finally, autonomous systems can see their priority improved by improving their feasibility. Specifically, you can add humans in the loop or reduce the system\u2019s natural autonomy to improve its feasibility. In the case of self-driving cars, many companies add safety drivers as guardrails to improve autonomous systems. In Voyage \u2019s case, they take a more dramatic approach of constraining the problem for the autonomous system: they only run self-driving cars in senior living communities, a narrow subset of the broader self-driving problem. 5 - Metrics So far, we\u2019ve talked about the overall ideas around picking projects and structuring them based on their archetypes and the specific considerations that go into them. Now, we\u2019ll shift gears and be a little more tactical to focus on metrics and baselines, which will help you execute projects more effectively. Choosing a Metric Metrics help us evaluate models . There\u2019s a delicate balance between the real world (which is always messy and multifaceted) and the machine learning paradigm (which optimizes a single metric) in choosing a metric. In practical production settings, we often care about multiple dimensions of performance (i.e., accuracy, speed, cost, etc.). The challenge is to reconcile all the possible evaluation methods with the reality that ML systems work best at optimizing a single number. How can we balance these competing needs in building an ML product? As you start evaluating models, choose a single metric to focus on first , such as precision, accuracy, recall, etc. This can serve as an effective first filter of performance. Subsequently, you can put together a formula that combines all the metrics you care about. Note that it\u2019s important to be flexible and regularly update this formula as your models or the requirements for the product change. Combining Metrics Two simple ways of combining metrics into a formula are averaging and thresholding . Averaging is less common but easy and intuitive; you can just take a simple average or a weighted average of the model\u2019s metrics and pick the highest average. More practically, you can apply a threshold evaluation to the model\u2019s metrics. In this method, out of n evaluation metrics, you threshold n-1 and optimize the nth metric. For example, if we look at a model\u2019s precision, memory requirement, and cost to train, we might threshold the memory requirement (no more than X MB) and the cost (no more than $X) and optimize precision (as high as possible). As you choose which metrics to threshold and what to set their threshold values to, make sure to consider domain-specific needs and the actual values of the metrics (how good/bad they might be). 6 - Baselines In any product development process, setting expectations properly is vital. For machine learning products, baselines help us set expectations for how well our model will perform . In particular, baselines set a useful lower bound for our model\u2019s performance. What\u2019s the minimum expectation we should have for a model\u2019s performance? The better defined and clear the baseline is, the more useful it is for setting the right expectations. Examples of baselines are human performance on a similar task, state-of-the-art models, or even simple heuristics. Baselines are especially important for helping decide the next steps. Consider the example below of two models with the same loss curve but differing performance with respect to the baseline. Clearly, they require different action items! As seen below, on the left, where we are starting to approach or exceed the baseline, we need to be mindful of overfitting and perhaps incorporate regularization of some sort. On the right, where the baseline hugely exceeds our model\u2019s performance, we clearly have a lot of work to do to improve the model and address its underfitting. There are a number of sources to help us define useful baselines. Broadly speaking, there are external baselines (baselines defined by others) or internal baselines you can define yourself. With internal baselines, in particular, you don\u2019t need anything too complicated, or even something with ML! Simple tests like averaging across your dataset can help you understand if your model is achieving meaningful performance. If your model can\u2019t exceed a simple baseline like this, you might need to really re-evaluate the model. Human baselines are a particularly powerful form of baseline since we often seek to replace or augment human actions. In creating these baselines, note that there\u2019s usually an inverse relationship between the quality of the baseline and the ease of data collection. In a nutshell, the harder it is to get a human baseline, the better and more useful it probably is . For example, a Mechanical Turk-created baseline is easy to generate nowadays, but the quality might be hit or miss because of the variance in annotators. However, trained, specialized annotators can be hard to acquire, but the specificity of their knowledge translates into a great baseline. Choosing where to situate your baseline on this range, from low quality/easy to high quality/hard, depends on the domain. Concentrating data collection strategically, ideally in classes where the model is least performant, is a simple way of improving the quality of the baseline. TLDR Machine learning projects are iterative. Deploy something fast to begin the cycle. Choose projects with high impact and low cost of wrong predictions. The secret sauce to make projects work well is to build automated data flywheels. In the real world, you care about many things, but you should always have just one to work on. Good baselines help you invest your effort the right way. Further Resources Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d Andrej Kaparthy\u2019s \u201c Software 2.0 \u201d Agrawal, Gans, and Goldfarb\u2019s \u201c The Economics of AI \u201d Chip Huyen\u2019s \u201c Introduction to Machine Learning Systems Design \u201d Apple\u2019s \u201c Human-Interface Guidelines for Machine Learning \u201d Google\u2019s \u201c Rules of Machine Learning \u201d","title":"Lecture 5: ML Projects"},{"location":"spring2021/lecture-5/#lecture-5-ml-projects","text":"Learn how to set up Machine Learning projects like a pro. This includes an understanding of the ML lifecycle, an acute mind of the feasibility and impact, an awareness of the project archetypes, and an obsession with metrics and baselines.","title":"Lecture 5: ML Projects"},{"location":"spring2021/lecture-5/#video","text":"","title":"Video"},{"location":"spring2021/lecture-5/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-5/#detailed-notes","text":"By James Le and Vishnu Rachakonda","title":"Detailed Notes"},{"location":"spring2021/lecture-5/#1-why-do-ml-projects-fail","text":"Based on a report from TechRepublic a few years back, despite increased interest in adopting machine learning (ML) in the enterprise, 85% of machine learning projects ultimately fail to deliver on their intended promises to business. Failure can happen for many reasons; however, a few glaring dangers will cause any AI project to crash and burn. ML is still very much a research endeavor. Therefore it is very challenging to aim for a 100% success rate. Many ML projects are technically infeasible or poorly scoped. Many ML projects never leap production, thus getting stuck at the prototype phase. Many ML projects have unclear success criteria because of a lack of understanding of the value proposition. Many ML projects are poorly managed because of a lack of interest from leadership.","title":"1 - Why Do ML Projects Fail?"},{"location":"spring2021/lecture-5/#2-lifecycle","text":"It\u2019s essential to understand what constitutes all of the activities in a machine learning project. Typically speaking, there are four major phases: Planning and Project Setup : At this phase, we want to decide the problem to work on, determine the requirements and goals, figure out how to allocate resources properly, consider the ethical implications, etc. Data Collection and Labeling : At this phase, we want to collect training data and potentially annotate them with ground truth, depending on the specific sources where they come from. We may find that it\u2019s too hard to get the data, or it might be easier to label for a different task. If that\u2019s the case, go back to phase 1. Model Training and Model Debugging : At this phase, we want to implement baseline models quickly, find and reproduce state-of-the-art methods for the problem domain, debug our implementation, and improve the model performance for specific tasks. We may realize that we need to collect more data or that labeling is unreliable (thus, go back to phase 2). Or we may recognize that the task is too challenging and there is a tradeoff between project requirements (thus, go back to phase 1). Model Deploying and Model Testing : At this phase, we want to pilot the model in a constrained environment (i.e., in the lab), write tests to prevent regressions, and roll the model into production. We may see that the model doesn\u2019t work well in the lab, so we want to keep improving the model\u2019s accuracy (thus, go back to phase 3). Or we may want to fix the mismatch between training data and production data by collecting more data and mining hard cases (thus go back to phase 2). Or we may find out that the metric picked doesn\u2019t actually drive downstream user behavior, and/or performance in the real world isn\u2019t great. In such situations, we want to revisit the projects\u2019 metrics and requirements (thus, go back to phase 1). Besides the per-project activities mentioned above, there are two other things that any ML team will need to solve across any projects they get involved with: (1) building the team and hiring people; and (2) setting up infrastructure and tooling to build ML systems repeatedly and at scale. Additionally, it might be useful to understand state-of-the-art results in your application domain so that you know what\u2019s possible and what to try next.","title":"2 - Lifecycle"},{"location":"spring2021/lecture-5/#3-prioritizing-projects","text":"To prioritize projects to work on, you want to find high-impact problems and assess the potential costs associated with them. The picture below shows a general framework that encourages us to target projects with high impact and high feasibility.","title":"3 - Prioritizing Projects"},{"location":"spring2021/lecture-5/#high-impact","text":"There are no silver bullets to find high-impact ML problems to work on, but here are a few useful mental models: Where can you take advantage of cheap prediction? Where is there friction in your product? Where can you automate complicated manual processes? What are other people doing?","title":"High Impact"},{"location":"spring2021/lecture-5/#cheap-prediction","text":"In the book \u201c Prediction Machines ,\u201d the authors (Ajay Agrawal, Joshua Gans, and Avi Goldfarb) come up with an excellent mental model on the economics of Artificial Intelligence: As AI reduces the cost of prediction and prediction is central for decision making, cheap predictions would be universal for problems across business domains . Therefore, you should look for projects where cheap predictions will have a huge business impact.","title":"Cheap Prediction"},{"location":"spring2021/lecture-5/#product-needs","text":"Another lens is to think about what your product needs. In the article \u201c Three Principles for Designing ML-Powered Products ,\u201d the Spotify Design team emphasizes the importance of building ML from a product perspective and looking for parts of the product experience with high friction . Automating those parts is exactly where there is a lot of impact for ML to make your business better.","title":"Product Needs"},{"location":"spring2021/lecture-5/#ml-strength","text":"In his popular blog post \u201c Software 2.0 ,\u201d Andrej Karpathy contrasts software 1.0 (which are traditional programs with explicit instructions) and software 2.0 (where humans specify goals, while the algorithm searches for a program that works). Software 2.0 programmers work with datasets, which get compiled via optimization\u200a\u2014\u200awhich works better, more general, and less computationally expensive. Therefore, you should look for complicated rule-based software where we can learn the rules instead of programming them.","title":"ML Strength"},{"location":"spring2021/lecture-5/#inspiration-from-others","text":"Instead of reinventing the wheel, you can look at what other companies are doing. In particular, check out papers from large frontier organizations (Google, Facebook, Nvidia, Netflix, etc.) and blog posts from top earlier-stage companies (Uber, Lyft, Spotify, Stripe, etc.). Here is a list of excellent ML use cases to check out (credit to Chip Huyen\u2019s ML Systems Design Lecture 2 Note ): Human-Centric Machine Learning Infrastructure at Netflix (Ville Tuulos, InfoQ 2019) 2020 state of enterprise machine learning (Algorithmia, 2020) Using Machine Learning to Predict Value of Homes On Airbnb (Robert Chang, Airbnb Engineering & Data Science, 2017) Using Machine Learning to Improve Streaming Quality at Netflix (Chaitanya Ekanadham, Netflix Technology Blog, 2018) 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com (Bernardi et al., KDD, 2019) How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach (Gabriel Aldamiz, HackerNoon, 2018) Machine Learning-Powered Search Ranking of Airbnb Experiences (Mihajlo Grbovic, Airbnb Engineering & Data Science, 2019) From shallow to deep learning in fraud (Hao Yi Ong, Lyft Engineering, 2018) Space, Time and Groceries (Jeremy Stanley, Tech at Instacart, 2017) Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning (Brad Neuberg, Dropbox Engineering, 2017) Scaling Machine Learning at Uber with Michelangelo (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019) Spotify\u2019s Discover Weekly: How machine learning finds your new music (Sophia Ciocca, 2017)","title":"Inspiration From Others"},{"location":"spring2021/lecture-5/#high-feasibility","text":"The three primary cost drivers of ML projects in order of importance are data availability, accuracy requirement, and problem difficulty.","title":"High Feasibility"},{"location":"spring2021/lecture-5/#data-availability","text":"Here are the questions you need to ask concerning the data availability: How hard is it to acquire data? How expensive is data labeling? How much data will be needed? How stable is the data? What are the data security requirements?","title":"Data Availability"},{"location":"spring2021/lecture-5/#accuracy-requirement","text":"Here are the questions you need to ask concerning the accuracy requirement: How costly are wrong predictions? How frequently does the system need to be right to be useful? What are the ethical implications? It is worth noting that ML project costs tend to scale super-linearly in the accuracy requirement. The fundamental reason is that you typically need a lot more data and more high-quality labels to achieve high accuracy numbers.","title":"Accuracy Requirement"},{"location":"spring2021/lecture-5/#problem-difficulty","text":"Here are the questions you need to ask concerning the problem difficulty: Is the problem well-defined? Is there good published work on similar problems? What are the computing requirements? Can a human do it? So what\u2019s still hard in machine learning? As a caveat, it\u2019s historically very challenging to predict what types of problems will be difficult for ML to solve in the future. But generally speaking, both unsupervised learning and reinforcement learning are still hard, even though they show promise in limited domains where tons of data and compute are available. Zooming into supervised learning , here are three types of hard problems: Output is complex: These are problems where the output is high-dimensional or ambiguous. Examples include 3D reconstruction, video prediction, dialog systems, open-ended recommendation systems, etc. Reliability is required: These are problems where high precision and robustness are required. Examples include systems that can fail safely in out-of-distribution scenarios, is robust to adversarial attacks, or needs to tackle highly precise tasks. Generalization is required: These are problems with out-of-distribution data or in the domains of reasoning, planning, and causality. Examples include any systems for self-driving vehicles or any systems that deal with small data. Finally, this is a nice checklist for you to run an ML feasibility assessment: Are you sure that you need ML at all? Put in the work upfront to define success criteria with all of the stakeholders. Consider the ethics of using ML. Do a literature review. Try to build a labeled benchmark dataset rapidly. Build a minimal viable product with manual rules Are you \u201creally sure\u201d that you need ML at all?","title":"Problem Difficulty"},{"location":"spring2021/lecture-5/#4-archetypes","text":"So far, we\u2019ve talked about the lifecycle and the impact of all machine learning projects. Ultimately, we generally want these projects, or applications of machine learning, to be useful for products. As we consider how ML can be applied in products, it\u2019s helpful to note that there are common machine learning product archetypes or recurrent patterns through which machine learning is applied to products. You can think of these as \u201cmental models\u201d you can use to assess your project and easily prioritize the needed resources. There are three common archetypes in machine learning projects: Software 2.0 , Human-in-the-loop , and autonomous systems . They are shown in the table below, along with common examples and questions. We\u2019ll dive deeper into each. Archetype Examples Questions Software 2.0 - Improve code completion in IDE - Build customized recommendation system - Build a better video game AI - Do your models truly improve performance? - Does performance improvement generate business value? - Do performance improvements lead to a data flywheel? Human-in-the-loop - Turn sketches into slides - Email auto-completion - Help radiologists do job faster - How good does the system need to be to be useful? - How can you collect enough data to make it good? Autonomous Systems - Full self-driving - Automated customer support - Automated website design - What is an acceptable failure rate for the system? - How can you guarantee that it won\u2019t exceed the failure rate? - How inexpensively can you label data from the system?","title":"4 - Archetypes"},{"location":"spring2021/lecture-5/#software-20","text":"Software 2.0, which we previously alluded to from the Karpathy article , is defined as \u201c augmenting existing rules-based or deterministic software with machine learning, a probabilistic approach .\u201d Examples of this are taking a code completer in an IDE and improving the experience for the user by adding an ML component. Rather than suggesting a command based solely on the leading characters the programmer has written, you might add a model that suggests commands based on previous commands the programmer has written. As you build a software 2.0 project, strongly consider the concept of the data flywheel . For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It\u2019s a classic virtuous cycle and truly the gold standard for ML projects. In embarking on creating a data flywheel, critically consider where the model could fail in relation to your product. For example, do more users lead to collecting more data that is useful for improving your model? An actual system needs to be set up to capture this data and ensure that it's meaningful for the ML lifecycle. Furthermore, consider whether more data will lead to a better model (your job as an ML practitioner) or whether a better model and better predictions will actually lead to making the product better. Ideally, you should have a quantitative assessment of what makes your product \u201cbetter\u201d and map model improvement to it.","title":"Software 2.0"},{"location":"spring2021/lecture-5/#human-in-the-loop-hil","text":"HIL systems are defined as machine learning systems where the output of your model will be reviewed by a human before being executed in the real world . For example, consider translating sketches into slides. An ML algorithm can take a sketch\u2019s input and suggest to a user a particular slide design. Every output of the ML model is considered and executed upon by a human, who ultimately has to decide on the slide\u2019s design.","title":"Human-in-the-Loop (HIL)"},{"location":"spring2021/lecture-5/#autonomous-systems","text":"Autonomous systems are defined as machine learning systems where the system itself makes decisions or engages in outputs that are almost never reviewed by a human . Canonically, consider the self-driving car!","title":"Autonomous Systems"},{"location":"spring2021/lecture-5/#feasibility","text":"Let\u2019s discuss how the product archetypes relate back to project priority. In terms of feasibility and impact, the two axes on which we consider priority, software 2.0 tends to have high feasibility but potentially low impact. The existing system is often being optimized rather than wholly replaced. However, this status with respect to priority is not static by any means. Building a data flywheel into your software 2.0 project can improve your product\u2019s impact by improving the model\u2019s performance on the task and future ones. In the case of human-in-the-loop systems, their feasibility and impact sit squarely in between autonomous systems and software 2.0. HIL systems, in particular, can benefit disproportionately in their feasibility and impact from effective product design, which naturally takes into account how humans interact with technology and can mitigate risks for machine learning model behavior. Consider how the Facebook photo tagging algorithm is implemented. Rather than tagging the user itself, the algorithm frequently asks the user to tag themselves. This effective product design allows the model to perform more effectively in the user\u2019s eye and reduces the impact of false classifications. Grammarly similarly solicits user input as part of its product design through offering explanations. Finally, recommender systems also implement this idea. In general, good product design can smooth the rough edges of ML (check out the concept of designing collaborative AI ). There are industry-leading resources that can help you merge product design and ML. Apple\u2019s ML product design guidelines suggest three key questions to anyone seeking to put ML into a product: What role does ML play in your product? How can you learn from your users? How should your app handle mistakes? Associated with each question is a set of design paradigms that help address the answers to each question. There are similarly great resources from Microsoft and Spotify . Finally, autonomous systems can see their priority improved by improving their feasibility. Specifically, you can add humans in the loop or reduce the system\u2019s natural autonomy to improve its feasibility. In the case of self-driving cars, many companies add safety drivers as guardrails to improve autonomous systems. In Voyage \u2019s case, they take a more dramatic approach of constraining the problem for the autonomous system: they only run self-driving cars in senior living communities, a narrow subset of the broader self-driving problem.","title":"Feasibility"},{"location":"spring2021/lecture-5/#5-metrics","text":"So far, we\u2019ve talked about the overall ideas around picking projects and structuring them based on their archetypes and the specific considerations that go into them. Now, we\u2019ll shift gears and be a little more tactical to focus on metrics and baselines, which will help you execute projects more effectively.","title":"5 - Metrics"},{"location":"spring2021/lecture-5/#choosing-a-metric","text":"Metrics help us evaluate models . There\u2019s a delicate balance between the real world (which is always messy and multifaceted) and the machine learning paradigm (which optimizes a single metric) in choosing a metric. In practical production settings, we often care about multiple dimensions of performance (i.e., accuracy, speed, cost, etc.). The challenge is to reconcile all the possible evaluation methods with the reality that ML systems work best at optimizing a single number. How can we balance these competing needs in building an ML product? As you start evaluating models, choose a single metric to focus on first , such as precision, accuracy, recall, etc. This can serve as an effective first filter of performance. Subsequently, you can put together a formula that combines all the metrics you care about. Note that it\u2019s important to be flexible and regularly update this formula as your models or the requirements for the product change.","title":"Choosing a Metric"},{"location":"spring2021/lecture-5/#combining-metrics","text":"Two simple ways of combining metrics into a formula are averaging and thresholding . Averaging is less common but easy and intuitive; you can just take a simple average or a weighted average of the model\u2019s metrics and pick the highest average. More practically, you can apply a threshold evaluation to the model\u2019s metrics. In this method, out of n evaluation metrics, you threshold n-1 and optimize the nth metric. For example, if we look at a model\u2019s precision, memory requirement, and cost to train, we might threshold the memory requirement (no more than X MB) and the cost (no more than $X) and optimize precision (as high as possible). As you choose which metrics to threshold and what to set their threshold values to, make sure to consider domain-specific needs and the actual values of the metrics (how good/bad they might be).","title":"Combining Metrics"},{"location":"spring2021/lecture-5/#6-baselines","text":"In any product development process, setting expectations properly is vital. For machine learning products, baselines help us set expectations for how well our model will perform . In particular, baselines set a useful lower bound for our model\u2019s performance. What\u2019s the minimum expectation we should have for a model\u2019s performance? The better defined and clear the baseline is, the more useful it is for setting the right expectations. Examples of baselines are human performance on a similar task, state-of-the-art models, or even simple heuristics. Baselines are especially important for helping decide the next steps. Consider the example below of two models with the same loss curve but differing performance with respect to the baseline. Clearly, they require different action items! As seen below, on the left, where we are starting to approach or exceed the baseline, we need to be mindful of overfitting and perhaps incorporate regularization of some sort. On the right, where the baseline hugely exceeds our model\u2019s performance, we clearly have a lot of work to do to improve the model and address its underfitting. There are a number of sources to help us define useful baselines. Broadly speaking, there are external baselines (baselines defined by others) or internal baselines you can define yourself. With internal baselines, in particular, you don\u2019t need anything too complicated, or even something with ML! Simple tests like averaging across your dataset can help you understand if your model is achieving meaningful performance. If your model can\u2019t exceed a simple baseline like this, you might need to really re-evaluate the model. Human baselines are a particularly powerful form of baseline since we often seek to replace or augment human actions. In creating these baselines, note that there\u2019s usually an inverse relationship between the quality of the baseline and the ease of data collection. In a nutshell, the harder it is to get a human baseline, the better and more useful it probably is . For example, a Mechanical Turk-created baseline is easy to generate nowadays, but the quality might be hit or miss because of the variance in annotators. However, trained, specialized annotators can be hard to acquire, but the specificity of their knowledge translates into a great baseline. Choosing where to situate your baseline on this range, from low quality/easy to high quality/hard, depends on the domain. Concentrating data collection strategically, ideally in classes where the model is least performant, is a simple way of improving the quality of the baseline.","title":"6 - Baselines"},{"location":"spring2021/lecture-5/#tldr","text":"Machine learning projects are iterative. Deploy something fast to begin the cycle. Choose projects with high impact and low cost of wrong predictions. The secret sauce to make projects work well is to build automated data flywheels. In the real world, you care about many things, but you should always have just one to work on. Good baselines help you invest your effort the right way.","title":"TLDR"},{"location":"spring2021/lecture-5/#further-resources","text":"Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d Andrej Kaparthy\u2019s \u201c Software 2.0 \u201d Agrawal, Gans, and Goldfarb\u2019s \u201c The Economics of AI \u201d Chip Huyen\u2019s \u201c Introduction to Machine Learning Systems Design \u201d Apple\u2019s \u201c Human-Interface Guidelines for Machine Learning \u201d Google\u2019s \u201c Rules of Machine Learning \u201d","title":"Further Resources"},{"location":"spring2021/lecture-6/","text":"Lecture 6: Infrastructure & Tooling Detailed Notes Detailed notes are available thanks to James Le and Vishnu Rachakonda. Video Slides PDF Download Notes In this video, you'll get exposed to the core areas of ML infrastructure and tools landscape. Then, you'll also see a comprehensive overview of tools and platforms for the training/evaluation bucket - which includes software engineering, computing needs, resource management, frameworks and distributed training, experiment management, hyperparameter optimization, and end-to-end solutions. 00:00 - Introduction 00:24 - The Dream vs. The Reality for ML Practitioners 03:18 - The 3 Buckets of ML Infrastructure/Tooling Landscape 06:20 - Software Engineering 17:06 - Compute Hardware 36:55 - Resource Management 41:40 - Frameworks and Distributed Training 52:48 - Experiment Management 55:43 - Hyperparameter Tuning 59:00 - \"All-In-One\" Solutions 01:06:24 - Follow us on Twitter for #ToolingTuesday!","title":"Lecture 6: Infrastructure & Tooling"},{"location":"spring2021/lecture-6/#lecture-6-infrastructure-tooling","text":"Detailed Notes Detailed notes are available thanks to James Le and Vishnu Rachakonda.","title":"Lecture 6: Infrastructure &amp; Tooling"},{"location":"spring2021/lecture-6/#video","text":"","title":"Video"},{"location":"spring2021/lecture-6/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-6/#notes","text":"In this video, you'll get exposed to the core areas of ML infrastructure and tools landscape. Then, you'll also see a comprehensive overview of tools and platforms for the training/evaluation bucket - which includes software engineering, computing needs, resource management, frameworks and distributed training, experiment management, hyperparameter optimization, and end-to-end solutions. 00:00 - Introduction 00:24 - The Dream vs. The Reality for ML Practitioners 03:18 - The 3 Buckets of ML Infrastructure/Tooling Landscape 06:20 - Software Engineering 17:06 - Compute Hardware 36:55 - Resource Management 41:40 - Frameworks and Distributed Training 52:48 - Experiment Management 55:43 - Hyperparameter Tuning 59:00 - \"All-In-One\" Solutions 01:06:24 - Follow us on Twitter for #ToolingTuesday!","title":"Notes"},{"location":"spring2021/notebook-1/","text":"Notebook: Coding a neural net Video In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron Follow Along Google Colab","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#notebook-coding-a-neural-net","text":"","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#video","text":"In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron","title":"Video"},{"location":"spring2021/notebook-1/#follow-along","text":"Google Colab","title":"Follow Along"},{"location":"spring2021/projects/","text":"","title":"Projects"},{"location":"spring2021/synchronous/","text":"Synchronous Online Course For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion How do I know if I am in this course? If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack. Teaching Assistants This course is only possible with the support of our amazing TAs: Head TA James Le runs Data Relations for Superb AI and contributes to Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Previously, he co-managed TEFQX . Nadia Ahmed is a machine learning researcher with The Frontier Development Lab and Trillium Technologies in remote sensing for severe weather and flood events. Andrew Mendez is a Senior Machine Learning Engineer at Clarifai, developing large scale computer vision and machine learning systems for the public sector. Previously he was a ML Engineer at CACI. Vishnu Rachakonda is a Machine Learning Engineer at Tesseract Health, a retinal imaging company, where he builds machine learning models for workflow augmentation and diagnostics in on-device and cloud use cases. Chester Chen is the Director of Data Science Engineering at GoPro. He also founded the SF Big Analytics Meetup. Schedule While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project. Logistics All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own. Projects The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning. If you want to find a partner, please post in the #spring2021-projects Slack channel with your idea or just that you're available to pair up. Project proposals are due on Gradescope a few weeks into the course. Please read more information about the projects . Certificate Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn. Time Commitment On average, expect to spend 5-10 hours per week on the course.","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#synchronous-online-course","text":"For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#how-do-i-know-if-i-am-in-this-course","text":"If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack.","title":"How do I know if I am in this course?"},{"location":"spring2021/synchronous/#teaching-assistants","text":"This course is only possible with the support of our amazing TAs: Head TA James Le runs Data Relations for Superb AI and contributes to Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Previously, he co-managed TEFQX . Nadia Ahmed is a machine learning researcher with The Frontier Development Lab and Trillium Technologies in remote sensing for severe weather and flood events. Andrew Mendez is a Senior Machine Learning Engineer at Clarifai, developing large scale computer vision and machine learning systems for the public sector. Previously he was a ML Engineer at CACI. Vishnu Rachakonda is a Machine Learning Engineer at Tesseract Health, a retinal imaging company, where he builds machine learning models for workflow augmentation and diagnostics in on-device and cloud use cases. Chester Chen is the Director of Data Science Engineering at GoPro. He also founded the SF Big Analytics Meetup.","title":"Teaching Assistants"},{"location":"spring2021/synchronous/#schedule","text":"While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project.","title":"Schedule"},{"location":"spring2021/synchronous/#logistics","text":"All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own.","title":"Logistics"},{"location":"spring2021/synchronous/#projects","text":"The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning. If you want to find a partner, please post in the #spring2021-projects Slack channel with your idea or just that you're available to pair up. Project proposals are due on Gradescope a few weeks into the course. Please read more information about the projects .","title":"Projects"},{"location":"spring2021/synchronous/#certificate","text":"Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn.","title":"Certificate"},{"location":"spring2021/synchronous/#time-commitment","text":"On average, expect to spend 5-10 hours per week on the course.","title":"Time Commitment"}]}